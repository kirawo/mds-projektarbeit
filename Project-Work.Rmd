---
title: "Stroke Risk Factors"
author: "Kira Wolff"
date: "11 4 2021"
output: pdf_document
bibliography: bibliography.bib
---
# Introduction

After a stroke occurs, every minute counts [@darehed2020]. The sooner the affected person 
receives medical care, the higher the chances of a good recovery and little 
subsequent damages. Thus it is key to recocgnize a stroke occured. Besides
knowledge about the visible symptoms of a stroke, knowledge about who is at 
high(er) risk to suffer a stroke can help channeling valuable attention 
resources on the these people.

One opportunity to identify the risk factors is ...

## Research Question

The goal of this project is to identify medical and/or lifestyle factors that 
are associated with stroke occurence. These factors can potentially be used to
assess the risk of someone experiencing a stroke.

```{r setup, include=FALSE}
# chunk options
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = FALSE, 
                      message = FALSE, include=FALSE)

set.seed(30051994) # for reproducibility
```

```{r packages}
library(tidyverse) # data handling
library(ggpubr) # arrange plots
library(caret) # modeling methods
library(randomForest) # to use random forest in caret
library(rpart.plot) # tree plots
library(knitr) # table design
```

gridExtra vllt raus, vllt nur ggpubr notwending


# Method

To answer the research question, data about people with and without a stroke in
their medical history will be used. The goal is to build a model that classifies
the people correctly as stroke patients and people without stroke history. This
model could then be applied to other people about whom basic demographic,
medical and lifestyle data is known. If the model classifies them as stroke
patients although they did not experience one, they should be treated as high
risk.

As common procedure for binary classification problems, a logistic regression 
will be conducted. 
Considering potentially more complex connections 
between the variables, further modeling approaches will be used. Firstly, a regularization
term in form of Elastic Net will be added which is especially sensible in 
prediction contexts as it reduces overfitting. Elastic Net was choosen instead of
LASSO or Ridge regularization because it enables a sort of compromise between the 
other methods, as both parameters can be tuned. 
Secondly, to consider the case that the relationship of the variables is better 
represented by a more complex function, support vector machines (SVMs) with a 
linear kernel will be applied. 
Finally, a classification tree using the random forest algorithm will be applied. 
This might be 
especially useful to identify the most important/impactful covariates, that is, 
predictors. 

To train the models and tune the parameters, repeated 5-fold cross-validation will be used. Additionally, the final
models' performance will be tested with a subset of the data that is not part of the training, consisting of 20% of the original dataset. 
The best model will be identified via accuracy
as performance criterium. Sensitivity and Specifity will also be considered, as
well as the general complexity of the model - in case of near identical performance,
simpler models will be chosen.

Finally, the best model will be evaluated regarding the predictors. If possible,
a p-value of 0.05 will be considered as a significant result.

## Implementation

The analysis will be conducted using R version 4.0.3 [@rcoreteam2020] in RStudio
version 1.3.1093 [@rstudio2020]. Data will be handled via _tidyverse_ [@tidyverse2019], primarly with _dplyr_ 
 [dplyr2020], and plotted with _ggplot2_ [@ggplot2016] and _ggpubr_ [@ggpubr2020].
The models will be implemented using _caret_ [@caret2020] and _randomForest_ [...]. For resampling, the 
methods offered by _caret_ will be used. Data will be preprocessed by centering and scaling before building the models.

- preprocess

## Data

The data comes from the publicly accessible "Stroke Prediction Dataset" which was
published by the user fedesoriano on kaggle.net in January of 2021 [@strokedata].
Unfortunately, no further information about the validity of the data is known.
The dataset contains information about demographic, medical and lifestyle data 
from >5000 people.
Missing Data were handled via single imputation by predicting the missing values
with a linear regression model

# Results

```{r transform data}
# define classes
stroke <- read.csv("healthcare-dataset-stroke-data.csv") %>% 
  mutate(id = as.factor(id),
         gender = as.factor(gender),
         age = as.numeric(age),
         hypertension = as.factor(hypertension),
         heart_disease = as.factor(heart_disease),
         ever_married = as.factor(ever_married),
         work_type = as.factor(work_type),
         Residence_type = as.factor(Residence_type),
         avg_glucose_level = as.numeric(avg_glucose_level),
         bmi = as.numeric(bmi),
         smoking_status = as.factor(smoking_status),
         stroke = as.factor(stroke)
         )
```

```{r missing data}
# function to ease identifying variables with missing data
any.na <- function(col){
  any(is.na(col))
}

# identify variables with missing data
apply(stroke[,1:length(names(stroke))], 2, FUN="any.na")


# imputation of missing data via regression
## create dataset without missing data
stroke.complete <- stroke %>% 
  na.omit(bmi)

## build model
lm.bmi <- lm(bmi ~ gender + age + hypertension + heart_disease + ever_married + 
               work_type + Residence_type + avg_glucose_level + 
               smoking_status + stroke,
             data = stroke.complete)

## create dataset with missing subjects
stroke.na <- stroke %>% 
  filter(is.na(bmi))

## predict bmi
bmi.pred <- predict(lm.bmi, stroke.na)

## substitute missing data with predicted data
stroke.na$bmi <- bmi.pred

stroke.clean <- rbind(stroke.na, stroke.complete)
```


## Subjects

```{r calculations for descriptives}
nsub <- length(stroke.clean$id)

age.min <- min(stroke.clean$age)
age.max <- max(stroke.clean$age)
age.mean <- mean(stroke.clean$age)
age.sd <- sd(stroke.clean$age)

fem.perc <- round(prop.table(table(stroke.clean$gender))[1]*100,2)
men.perc <- round(prop.table(table(stroke.clean$gender))[2]*100,2)
div.perc <- round(prop.table(table(stroke.clean$gender))[3]*100,2)

gluc.mean <- mean(stroke.clean$avg_glucose_level)
gluc.sd <- sd(stroke.clean$avg_glucose_level)

bmi.mean <- mean(stroke.clean$bmi)
bmi.sd <- sd(stroke.clean$bmi)

sick <- stroke.clean %>% 
  filter(hypertension == 1 | heart_disease == 1 | stroke == 1) %>% 
  summarize(n = n(),
            perc = round(n()/nsub*100,2))
```


The data represents information about `r nsub` subjects. The sample has a large
age range from <1 to `r age.max` years with a mean of `r round(age.mean, 2)`
years (_SD_ = `r round(age.sd, 2)`). It consists of `r fem.perc`% women,
`r men.perc`% men, and `r div.perc`% identifying with other labels.

As Fig. XXX shows, the majority of the sample has no history of hypertension, 
heart disease, stroke or elevated blood glucose level. Excluding the latter, `r sick$perc`%
are affected by at least one of the aforementioned diagnoses overall.

Vielleicht Demographic Data Figure weglassen weil im Text beschrieben.

```{r Data Description Plot prep}
# gender, hypertension, heart_disease, ever_married, work_type, residence_type,
# smoking_status, stroke

# sort variables
var.cat <- c("gender", "hypertension", "heart_disease", "ever_married", 
             "work_type", "Residence_type", "smoking_status", "stroke")
var.cont <- c("age", "avg_glucose_level", "bmi")

# functions to enable "looping" plot creation
plot.bar <- function(data, x){
  ggplot(data, aes_string(x=x))+
    geom_bar()+
    ylab("Count")+
    theme_minimal()+
    theme(legend.title = element_blank())
}

plot.hist <- function(data, x){
  ggplot(data, aes_string(x=x))+
    geom_histogram()+
    theme_minimal()
}

# create base plots
cat.plots <- lapply(var.cat, plot.bar, data=stroke.clean)
cont.plots <- lapply(var.cont, plot.hist, data=stroke.clean)

# refine base plots
plot.gen <- cat.plots[[1]]+
  xlab("Gender")+
  ggtitle("Demographic Data")

plot.hyp <- cat.plots[[2]]+
  xlab("Hypertension")+
  scale_x_discrete(labels = c("No", "Yes"))+
  ggtitle("Medical Data")

plot.heart <- cat.plots[[3]]+
  xlab("Heart Disease")+
  scale_x_discrete(labels = c("No", "Yes"))

plot.mar <- cat.plots[[4]]+
  xlab("Married (Lifetime)")+
  ggtitle("Lifestyle Data")

plot.work <- cat.plots[[5]]+
  xlab("Work Type")+
  scale_x_discrete(labels = c("At Home \nwith \nChildren", "Govern. \nJob",
                              "Never \nWorked", "Private \nSector", 
                              "Self-\nemployed"))

plot.res <- cat.plots[[6]]+
  xlab("Residence Type")

plot.smok <- cat.plots[[7]]+
  xlab("Smoking Status")+
  scale_x_discrete(labels = c("formerly\nsmoked", "never\nsmoked", 
                              "currently\nsmokes", "unknown"))

plot.stroke <- cat.plots[[8]]+
  xlab("Stroke")+
  scale_x_discrete(labels = c("No", "Yes"))

plot.age <- cont.plots[[1]]+
  xlab("Age")+
  annotate(geom="text", x=15, y=220, 
           label=paste("M =",
                       round(age.mean, 2),
                       "\nSD =",
                       round(age.sd,2)))


plot.gluc <- cont.plots[[2]]+
  xlab("Glucose Level")+
  annotate(geom="text", x=200, y=600, 
           label=paste("M =",
                       round(gluc.mean, 2),
                       "\nSD =",
                       round(gluc.sd,2)))

plot.bmi <- cont.plots[[3]]+
  xlab("BMI")+
  annotate(geom="text", x=75, y=750, 
           label=paste("M =",
                       round(bmi.mean, 2),
                       "\nSD =",
                       round(bmi.sd,2)))
```



```{r Plot Demographic, fig.height=3, include=FALSE}
ggarrange(plot.gen, plot.age)
# do not show plot in pdf because text covers information
```

```{r Plot Medical, fig.height=6, include=TRUE}
ggarrange(plot.hyp, plot.heart, plot.stroke, plot.gluc)
```

```{r Plot Lifestyle, fig.height=9, include=TRUE}
ggarrange(plot.mar, plot.work, plot.res, plot.smok, plot.bmi, ncol=2, nrow=3)
```


## Model Evaluation

```{r Data Description}
table(stroke.clean$stroke)

summary(stroke.clean)

sum(stroke.clean$hypertension == 1 | stroke.clean$heart_disease == 1 | stroke.clean$stroke.clean == 1)/length(stroke.clean$id)
```

```{r data splitting}
# use 80% of the data to fit and tune the models, test accuracy with other 20%
train.index <- sample(1:length(stroke.clean$id),
                      size = floor(length(stroke.clean$id)*0.8))

train.data <- stroke.clean[train.index,]
test.data <- stroke.clean[-train.index,]
```

```{r variable selection}
model <- stroke ~ gender + age + hypertension + heart_disease + ever_married +
  work_type + Residence_type + avg_glucose_level + bmi + smoking_status
```


Model as placeholder

```{r prepare cross-validation}
trctrl <- trainControl(method = "repeatedcv", 
                       number = 5,
                       repeats = 3)
```



```{r logistic regression}
# fit model
glm.train <- train(model,
                   train.data,
                   trControl = trctrl,
                   method = "glm",
                   preProcess = c("center", "scale"),
                   family = "binomial")

summary(glm.train)
glm.train

# predict test data
glm.preds <- predict(glm.train, test.data)

# compare prediction and real data
glm.mat <- confusionMatrix(glm.preds, test.data$stroke)

glm.mat

r <- recall(glm.preds, test.data$stroke)
p <- precision(glm.preds, test.data$stroke)

r <- 0/(58+0)
p <- 0/(0+0)

r*p/(r+p)

F_meas(glm.preds, test.data$stroke)
```


- Varianzaufklärung auslesen?

```{r regularized log regression}
# fit model
reg.train <- train(model,
                   train.data,
                   method = "glmnet",
                   preProcess = c("center", "scale"),
                   metric = "Accuracy",
                   tuneLength = 20,
                   family = "binomial")

# predict test data
reg.preds <- predict(reg.train, test.data)

# compare prediction and real data
reg.mat <- confusionMatrix(reg.preds, test.data$stroke)

```

- Methode glmnet begründen
- preProcess erwähnen und begründen
- metric Accuracy recherchieren
- Tuning auswerten: alpha -> welche Methode? Interpretation lambda
```{r SVM}
# fit model
svm.train <- train(model,
                   train.data,
                   method = "svmLinear",
                   trControl = trctrl,
                   preProcess = c("center", "scale"),
                   tuneLength = 10)

svm.train$bestTune

# predict test data
svm.preds <- predict(svm.train, test.data)

# compare prediction and real data
svm.mat <- confusionMatrix(svm.preds, test.data$stroke)

```

- Methode begründen: Von lssvmRadial zu svmlinear gewechselt, vorher hat es nicht
funktioniert. Was unterscheidet die Methoden?

```{r Random Forest}
# fit model
grid <- expand.grid(mtry = seq(0, 11, 0.5))

tree.train <- train(model,
                   train.data,
                   method = "rf",
                   trControl = trctrl,
                   preProcess = c("center", "scale"),
                   tuneLength = 10)

tree.train$bestTune

rpart.plot(tree.train$finalMode)

# predict test data
tree.preds <- predict(tree.train, test.data)

# compare prediction and real data
tree.mat <- confusionMatrix(tree.preds, test.data$stroke)



```

Accuracy mit tuneLength: 0.9496 bzw 0.9432 für pred
Acc mit grid von 0.003 bis 0.01: 0.953
Acc mit grid von 0.01 bis 0.05: 0.953, immer noch upper bound

- Methode rpart begründen

```{r model comparison, include=TRUE}
# list of all models
models <- list(glm.mat, reg.mat, svm.mat, tree.mat)

# function to ease extracting values from list
read.output <- function(list, info){
  if(info == "accuracy"){
    print(list$overall[1])
  } else { 
    if(info == "sensitivity"){
      print(list$byClass[1])
      } else {
        if(info == "specifity"){
          print(list$byClass[2])
        }
      }}}

# dataframe with models' results
model.comp <- data.frame(
  model = c("GLM", "Regularized Regression", "SVM", "Tree"),
  acc = sapply(models, read.output, info="accuracy"),
  sens = sapply(models, read.output, info="sensitivity"),
  spec = sapply(models, read.output, info="specifity")
)

kable(
  model.comp,
  #format = "latex",
  #booktabs = TRUE,
  col.names = c("Model", "Accuracy", "Sensitivity", "Specifity"),
  align = c("l", "c", "c", "c"),
  caption = "Comparison of the Models",
  digits = 3
)
```

Tabelle rundet an Stellen wo ich es nicht will
Bzw es wird nicht das geknittet was hier in der Tabelle steht?

## Predictor Evaluation

Since the GLM reached highest accuracy while being the simplest model, it will
be used to take a closer look at the predictors.

```{r GLM predictor}
glm.train
summary(glm.train)
```

The GLM with stroke as outcome yields age (...), hypertension (...), and 
glucose level (...) as significant predictors. 


# Discussion 

```{r calculating values for discussion}

```


According to accuracy as performance criterium, there is no difference between GLM,
regularized regression and SVM. Due to the principle to keep modeling as simple
as possible, as long as performance does not suffer, GLM is to be preferred in
this case. The more complex models did not improve prediction performance. 

On the first glance, the accuracy of XXXX seems to be a good result for the
goal to build a model to predict stroke occurence. Unfortunately, when
considering specifity as an additional performance criterium, the prediction does
not work well. Considering the data, the reason for this probably lies in the
ratio from subjects with to subjects without stroke which is XXXXX. This ratio 
leads to an unbalanced dataset, and accuracy is no longer a good performance
criterium (QUELLE????): Because of the high portion of healthy subjects, high 
accuracy is easy to acquire by always predicting "no stroke". There are
solutions for unbalanced datasets like Over- or Under-Sampling which unfortunately 
go beyond the scope of this project work.


Although the created models will thus not work well to predict future stroke
cases, the predictors can still be interpreted.

- Welches Modell am besten (nicht nur Acc, sondern auch Confusion Matrixes. 
Kommen zum gleichen Ergebnis?)
- Wie gut Modell
- Methodische Schlussfolgerung: Hier reicht es, Methode x zu verwenden
- Inhaltliche Schlussfolgerung:
- Welche Variablen meistes Potenzial - medizinisch vs. demografisch
- Variablen interpretieren - wann sollte man besonders drauf achten
- Falls Accuracy gut erscheint, aber Confusion Matrix zeigt dass nie gefunden: 
Liegt an Daten? Zu wenig Fälle. So wenige, dass Wette einfach auf "nicht" zu gehen
zu gut ist. Außerdem Stichprobe zu heterogen?
- Limitation: unbalanced, Datenherkunft unsicher
- Future: andere Methoden/Outcome die unbalanced berücksichtigen, etwas homogenere Daten 
weil Alter so wichtig -> auf Alter kontrollieren um andere Faktoren besser hervorzuheben


# References