---
title: "Stroke Risk Factors"
author: "Kira Wolff"
date: "11 4 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(tidyverse)
library(gridExtra) # arrange plots
library(caret) # modeling methods
library(knitr) # table design
```

# Method

- R Version, mit der gerechnet wurde
- Ergebnisse können mit github xx reproduziert werden

# Results

```{r clean data}
# noch NAs definieren
stroke.clean <- read.csv("healthcare-dataset-stroke-data.csv") %>% 
  mutate(id = as.factor(id),
         gender = as.factor(gender),
         age = as.numeric(age),
         hypertension = as.factor(hypertension),
         heart_disease = as.factor(heart_disease),
         ever_married = as.factor(ever_married),
         work_type = as.factor(work_type),
         Residence_type = as.factor(Residence_type),
         avg_glucose_level = as.numeric(avg_glucose_level),
         bmi = as.numeric(bmi),
         smoking_status = as.factor(smoking_status),
         stroke = as.factor(stroke)
         )
```
```{r imputation of NAs}

```

- bmi NA data

```{r Data Description categorial, eval=FALSE, include=FALSE}
# gender, hypertension, heart_disease, ever_married, work_type, residence_type,
# smoking_status, stroke

# collect categorial variables
var.cat <- c("gender", "hypertension", "heart_disease", "ever_married", 
             "work_type", "Residence_type", "smoking_status", "stroke")
plot.labels <- 1:8

# create function
plot.bar <- function(data, x, lab){
  ggplot(data, aes_string(x=x, fill=x))+
    geom_bar()+
    xlab(lab)+
    theme_minimal()
}

#apply function to all variables
cat.plots <- lapply(var.cat, plot.bar, data=stroke.clean, lab=plot.labels)
# labels werden gedruckt, Problem ist aber dass sie beim ersten Element bleiben
cat.plots <- lapply(1:length(var.cat), plot.bar, data=stroke.clean, lab=plot.labels[i])

# build grid of plots
do.call(grid.arrange, cat.plots)

cat.plots[[1]]+
  xlab("Gender")
# Ich könnte sie auch von Hand verschönern - habe ich Änderungen, die generalisiertbar sind?
```

```{r Data Description}
table(stroke.clean$stroke)

summary(stroke.clean)

hist(stroke.clean$age)

sum(stroke.clean$hypertension == 1 | stroke.clean$heart_disease == 1 | stroke.clean$stroke.clean == 1)/length(stroke.clean$id)
```

```{r data splitting}
# use 80% of the data to fit and tune the models, test accuracy with other 20%
train.index <- sample(1:length(stroke.clean$id),
                      size = floor(length(stroke.clean$id)*0.8))

train.data <- stroke.clean[train.index,]
test.data <- stroke.clean[-train.index,]
```

```{r variable selection}
model <- stroke ~ age + hypertension + heart_disease + ever_married
```


Model as placeholder

```{r prepare cross-validation}
set.seed(30051994) # for reproducibility

trctrl <- trainControl(method="cv", number=5)
```

- Argumentation cross-validation, k=?

```{r logistic regression}
# fit model
glm.train <- train(stroke ~ age,
                   train.data,
                   trControl = trctrl,
                   method = "glm",
                   family = "binomial")

summary(glm.train)
glm.train

# predict test data
glm.preds <- predict(glm.train, test.data)

# compare prediction and real data
glm.mat <- confusionMatrix(glm.preds, test.data$stroke)

glm.mat
```
- Sollte man hier auch zentrieren/standardisieren?
- Age erst mal als Platzhalter
- Varianzaufklärung auslesen?

```{r regularized log regression}
# fit model
reg.train <- train(model,
                   train.data,
                   method = "glmnet",
                   preProcess = c("center", "scale"),
                   metric = "Accuracy",
                   tuneLength = 10,
                   family = "binomial")

# Results
reg.train$bestTune

reg.train$results %>% 
  filter(alpha == reg.train$bestTune$alpha,
         lambda == reg.train$bestTune$lambda)

# predict test data
reg.preds <- predict(reg.train, test.data)

# compare prediction and real data
reg.mat <- confusionMatrix(reg.preds, test.data$stroke)

```

- Methode glmnet begründen
- preProcess erwähnen und begründen
- metric Accuracy recherchieren
- Tuning auswerten: alpha -> welche Methode? Interpretation lambda
```{r SVM}
# fit model
svm.train <- train(model,
                   train.data,
                   method = "svmLinear",
                   trControl = trctrl,
                   preProcess = c("center", "scale"),
                   tuneLength = 10)

svm.train$bestTune

# predict test data
svm.preds <- predict(svm.train, test.data)

# compare prediction and real data
svm.mat <- confusionMatrix(svm.preds, test.data$stroke)

```

- Methode begründen: Von lssvmRadial zu svmlinear gewechselt, vorher hat es nicht
funktioniert. Was unterscheidet die Methoden?

```{r Random Forest}
# fit model
tree.train <- train(model,
                   train.data,
                   method = "rpart",
                   trControl = trctrl,
                   preProcess = c("center", "scale"),
                   tuneLength = 10)

tree.train$bestTune

# predict test data
tree.preds <- predict(tree.train, test.data)

# compare prediction and real data
tree.mat <- confusionMatrix(tree.preds, test.data$stroke)

```


- Methode rpart begründen

```{r model comparison}
# list of all models
models <- list(glm.mat, reg.mat, svm.mat, tree.mat)

# function to ease extracting values from list
read.output <- function(list, info){
  if(info == "accuracy"){
    print(list$overall[1])
  } else { 
    if(info == "sensitivity"){
      print(list$byClass[1])
      } else {
        if(info == "specifity"){
          print(list$byClass[2])
        }
      }}}

# dataframe with models' results
model.comp <- data.frame(
  model = c("GLM", "Regularized Regression", "SVM", "Tree"),
  acc = sapply(models, read.output, info="accuracy"),
  sens = sapply(models, read.output, info="sensitivity"),
  spec = sapply(models, read.output, info="specifity")
)

# kable(
#   model.comp,
#   format = "latex",
#   booktabs = TRUE,
#   col.names = c("Model", "Accuracy", "Sensitivity", "Specifity"),
#   align = c(1, "c", "c", "c"),
#   caption = "Comparison of the Models"
# )
```


# Discussion 

- Welches Modell am besten (nicht nur Acc, sondern auch Confusion Matrixes. 
Kommen zum gleichen Ergebnis?)
- Wie gut Modell
- Methodische Schlussfolgerung: Hier reicht es, Methode x zu verwenden
- Inhaltliche Schlussfolgerung:
- Welche Variablen meistes Potenzial - medizinisch vs. demografisch
- Variablen interpretieren - wann sollte man besonders drauf achten
- Falls Accuracy gut erscheint, aber Confusion Matrix zeigt dass nie gefunden: 
Liegt an Daten? Zu wenig Fälle. So wenige, dass Wette einfach auf "nicht" zu gehen
zu gut ist. Außerdem Stichprobe zu heterogen?
